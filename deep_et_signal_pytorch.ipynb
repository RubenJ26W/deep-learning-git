{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d84eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: setuptools in /Users/veroniquemohy-cardoso/myenv/lib/python3.13/site-packages (from torch) (75.6.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/veroniquemohy-cardoso/myenv/lib/python3.13/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/veroniquemohy-cardoso/myenv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.0-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m339.2 kB/s\u001b[0m  \u001b[33m0:01:58\u001b[0mm0:00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, fsspec, filelock, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [torch]32m6/7\u001b[0m [torch]kx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.9.0 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.9.0 typing-extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b82071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154addb",
   "metadata": {},
   "source": [
    "# Opérations sur les tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "799502f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenseur de dimensions torch.Size([10, 5, 100]) et de type torch.float32\n"
     ]
    }
   ],
   "source": [
    "#Création d'un tenseur\n",
    "tenseur = torch.zeros((10,5,100)) # [10,5,100]\n",
    "tenseur = torch.ones((10,5,100)) # [10,5,100]\n",
    "tenseur = torch.rand((10,5,100)) # [10,5,100]\n",
    "\n",
    "tenseur_zeros = torch.zeros_like(tenseur) # [10,5,100]\n",
    "tenseur_copy = torch.clone(tenseur)\n",
    "\n",
    "print(f'Tenseur de dimensions {tenseur.shape} et de type {tenseur.dtype}')\n",
    "\n",
    "# Opération élémentaires\n",
    "tenseur2 = torch.rand((10,5,100))  # [10,5,100]\n",
    "tenseur_au_carre = tenseur**2  # [10,5,100]\n",
    "tenseur_exp = torch.exp(tenseur)  # [10,5,100]\n",
    "tenseur_somme = tenseur + tenseur2  # [10,5,100]\n",
    "tenseur_produit = tenseur * tenseur2  # [10,5,100]\n",
    "\n",
    "# Concaténation\n",
    "tenseur_concatene_axe0 = torch.cat([tenseur, tenseur2],axis=0)  # [20,5,100]\n",
    "tenseur_concatene_axe1 = torch.cat([tenseur, tenseur2],axis=1)  # [10,10,100]\n",
    "tenseur_concatene_axe2 = torch.cat([tenseur, tenseur2],axis=2)  # [10,5,200]\n",
    "\n",
    "tenseur_tile = torch.tile(tenseur,dims=(2,3,4))  # [20,15,400]\n",
    "\n",
    "# Opérations de réduction\n",
    "tenseur_reduit_axe0 = torch.mean(tenseur,axis=0)  # [5,100]\n",
    "tenseur_reduit_axe1 = torch.mean(tenseur,axis=1)  # [10,100]\n",
    "tenseur_reduit_axe2 = torch.mean(tenseur,axis=2)  # [10,5]\n",
    "\n",
    "tenseur_reduit_axe0_kd = torch.mean(tenseur,axis=0,keepdim=True)  # [1,5,100]\n",
    "\n",
    "# Manipulation sur les dimensions \n",
    "\n",
    "tenseur_21 = torch.transpose(tenseur,1,2) # [10,100,5]\n",
    "tenseur_flat = torch.flatten(tenseur) # [5000,]\n",
    "tenseur_newdim_0 = torch.unsqueeze(tenseur,dim=0) # [1,10,5,100]\n",
    "tenseur_newdim_1 = torch.unsqueeze(tenseur,dim=1) # [10,1,5,100]\n",
    "\n",
    "# Slicing \n",
    "tenseur_partie = tenseur[0,:,:] # [5,100]\n",
    "tenseur_partie = tenseur[0,...] # [5,100]\n",
    "tenseur_partie = tenseur[0:5,:,:] # [5,5,100]\n",
    "tenseur_partie = tenseur[0,:,0::2]# [10,5,50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531217b",
   "metadata": {},
   "source": [
    "# Optimisation d'un graphe de calculs\n",
    "Exemple : régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fec3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0:\n",
      "a: 0.0000, b: 0.0000, loss: 0.3189,  grad a: -0.3075, grad b: 0.0226\n",
      "Itération 1:\n",
      "a: 0.0308, b: -0.0023, loss: 0.3096,  grad a: -0.2899, grad b: 0.0485\n",
      "Itération 2:\n",
      "a: 0.0597, b: -0.0071, loss: 0.3011,  grad a: -0.2759, grad b: 0.0674\n",
      "Itération 3:\n",
      "a: 0.0873, b: -0.0139, loss: 0.2931,  grad a: -0.2646, grad b: 0.0812\n",
      "Itération 4:\n",
      "a: 0.1138, b: -0.0220, loss: 0.2856,  grad a: -0.2555, grad b: 0.0912\n",
      "Itération 5:\n",
      "a: 0.1393, b: -0.0311, loss: 0.2783,  grad a: -0.2480, grad b: 0.0982\n",
      "Itération 6:\n",
      "a: 0.1641, b: -0.0409, loss: 0.2712,  grad a: -0.2416, grad b: 0.1031\n",
      "Itération 7:\n",
      "a: 0.1883, b: -0.0512, loss: 0.2644,  grad a: -0.2361, grad b: 0.1063\n",
      "Itération 8:\n",
      "a: 0.2119, b: -0.0619, loss: 0.2577,  grad a: -0.2314, grad b: 0.1084\n",
      "Itération 9:\n",
      "a: 0.2351, b: -0.0727, loss: 0.2512,  grad a: -0.2271, grad b: 0.1096\n",
      "Itération 10:\n",
      "a: 0.2578, b: -0.0837, loss: 0.2449,  grad a: -0.2232, grad b: 0.1101\n",
      "Itération 11:\n",
      "a: 0.2801, b: -0.0947, loss: 0.2387,  grad a: -0.2196, grad b: 0.1102\n",
      "Itération 12:\n",
      "a: 0.3020, b: -0.1057, loss: 0.2327,  grad a: -0.2163, grad b: 0.1099\n",
      "Itération 13:\n",
      "a: 0.3237, b: -0.1167, loss: 0.2269,  grad a: -0.2131, grad b: 0.1093\n",
      "Itération 14:\n",
      "a: 0.3450, b: -0.1276, loss: 0.2212,  grad a: -0.2101, grad b: 0.1085\n",
      "Itération 15:\n",
      "a: 0.3660, b: -0.1384, loss: 0.2156,  grad a: -0.2072, grad b: 0.1076\n",
      "Itération 16:\n",
      "a: 0.3867, b: -0.1492, loss: 0.2102,  grad a: -0.2044, grad b: 0.1065\n",
      "Itération 17:\n",
      "a: 0.4072, b: -0.1599, loss: 0.2049,  grad a: -0.2017, grad b: 0.1054\n",
      "Itération 18:\n",
      "a: 0.4273, b: -0.1704, loss: 0.1998,  grad a: -0.1991, grad b: 0.1043\n",
      "Itération 19:\n",
      "a: 0.4472, b: -0.1808, loss: 0.1948,  grad a: -0.1965, grad b: 0.1031\n",
      "Itération 20:\n",
      "a: 0.4669, b: -0.1911, loss: 0.1899,  grad a: -0.1939, grad b: 0.1019\n",
      "Itération 21:\n",
      "a: 0.4863, b: -0.2013, loss: 0.1851,  grad a: -0.1914, grad b: 0.1007\n",
      "Itération 22:\n",
      "a: 0.5054, b: -0.2114, loss: 0.1805,  grad a: -0.1890, grad b: 0.0995\n",
      "Itération 23:\n",
      "a: 0.5243, b: -0.2214, loss: 0.1759,  grad a: -0.1866, grad b: 0.0983\n",
      "Itération 24:\n",
      "a: 0.5430, b: -0.2312, loss: 0.1715,  grad a: -0.1842, grad b: 0.0971\n",
      "Itération 25:\n",
      "a: 0.5614, b: -0.2409, loss: 0.1672,  grad a: -0.1819, grad b: 0.0959\n",
      "Itération 26:\n",
      "a: 0.5796, b: -0.2505, loss: 0.1630,  grad a: -0.1796, grad b: 0.0947\n",
      "Itération 27:\n",
      "a: 0.5975, b: -0.2599, loss: 0.1589,  grad a: -0.1773, grad b: 0.0935\n",
      "Itération 28:\n",
      "a: 0.6153, b: -0.2693, loss: 0.1549,  grad a: -0.1750, grad b: 0.0923\n",
      "Itération 29:\n",
      "a: 0.6328, b: -0.2785, loss: 0.1510,  grad a: -0.1728, grad b: 0.0912\n",
      "Itération 30:\n",
      "a: 0.6501, b: -0.2876, loss: 0.1472,  grad a: -0.1706, grad b: 0.0900\n",
      "Itération 31:\n",
      "a: 0.6671, b: -0.2966, loss: 0.1435,  grad a: -0.1685, grad b: 0.0889\n",
      "Itération 32:\n",
      "a: 0.6840, b: -0.3055, loss: 0.1399,  grad a: -0.1663, grad b: 0.0878\n",
      "Itération 33:\n",
      "a: 0.7006, b: -0.3143, loss: 0.1364,  grad a: -0.1642, grad b: 0.0867\n",
      "Itération 34:\n",
      "a: 0.7170, b: -0.3230, loss: 0.1330,  grad a: -0.1622, grad b: 0.0856\n",
      "Itération 35:\n",
      "a: 0.7332, b: -0.3315, loss: 0.1296,  grad a: -0.1601, grad b: 0.0845\n",
      "Itération 36:\n",
      "a: 0.7492, b: -0.3400, loss: 0.1264,  grad a: -0.1581, grad b: 0.0834\n",
      "Itération 37:\n",
      "a: 0.7651, b: -0.3483, loss: 0.1232,  grad a: -0.1561, grad b: 0.0824\n",
      "Itération 38:\n",
      "a: 0.7807, b: -0.3566, loss: 0.1201,  grad a: -0.1541, grad b: 0.0813\n",
      "Itération 39:\n",
      "a: 0.7961, b: -0.3647, loss: 0.1171,  grad a: -0.1522, grad b: 0.0803\n",
      "Itération 40:\n",
      "a: 0.8113, b: -0.3727, loss: 0.1142,  grad a: -0.1502, grad b: 0.0793\n",
      "Itération 41:\n",
      "a: 0.8263, b: -0.3807, loss: 0.1113,  grad a: -0.1483, grad b: 0.0783\n",
      "Itération 42:\n",
      "a: 0.8412, b: -0.3885, loss: 0.1085,  grad a: -0.1465, grad b: 0.0773\n",
      "Itération 43:\n",
      "a: 0.8558, b: -0.3962, loss: 0.1058,  grad a: -0.1446, grad b: 0.0763\n",
      "Itération 44:\n",
      "a: 0.8703, b: -0.4038, loss: 0.1031,  grad a: -0.1428, grad b: 0.0753\n",
      "Itération 45:\n",
      "a: 0.8845, b: -0.4114, loss: 0.1005,  grad a: -0.1410, grad b: 0.0744\n",
      "Itération 46:\n",
      "a: 0.8986, b: -0.4188, loss: 0.0980,  grad a: -0.1392, grad b: 0.0735\n",
      "Itération 47:\n",
      "a: 0.9126, b: -0.4262, loss: 0.0955,  grad a: -0.1374, grad b: 0.0725\n",
      "Itération 48:\n",
      "a: 0.9263, b: -0.4334, loss: 0.0931,  grad a: -0.1357, grad b: 0.0716\n",
      "Itération 49:\n",
      "a: 0.9399, b: -0.4406, loss: 0.0908,  grad a: -0.1340, grad b: 0.0707\n",
      "Itération 50:\n",
      "a: 0.9533, b: -0.4476, loss: 0.0885,  grad a: -0.1323, grad b: 0.0698\n",
      "Itération 51:\n",
      "a: 0.9665, b: -0.4546, loss: 0.0863,  grad a: -0.1306, grad b: 0.0689\n",
      "Itération 52:\n",
      "a: 0.9796, b: -0.4615, loss: 0.0841,  grad a: -0.1290, grad b: 0.0681\n",
      "Itération 53:\n",
      "a: 0.9925, b: -0.4683, loss: 0.0820,  grad a: -0.1273, grad b: 0.0672\n",
      "Itération 54:\n",
      "a: 1.0052, b: -0.4750, loss: 0.0800,  grad a: -0.1257, grad b: 0.0663\n",
      "Itération 55:\n",
      "a: 1.0178, b: -0.4817, loss: 0.0779,  grad a: -0.1241, grad b: 0.0655\n",
      "Itération 56:\n",
      "a: 1.0302, b: -0.4882, loss: 0.0760,  grad a: -0.1226, grad b: 0.0647\n",
      "Itération 57:\n",
      "a: 1.0424, b: -0.4947, loss: 0.0741,  grad a: -0.1210, grad b: 0.0639\n",
      "Itération 58:\n",
      "a: 1.0545, b: -0.5011, loss: 0.0722,  grad a: -0.1195, grad b: 0.0631\n",
      "Itération 59:\n",
      "a: 1.0665, b: -0.5074, loss: 0.0704,  grad a: -0.1180, grad b: 0.0623\n",
      "Itération 60:\n",
      "a: 1.0783, b: -0.5136, loss: 0.0686,  grad a: -0.1165, grad b: 0.0615\n",
      "Itération 61:\n",
      "a: 1.0899, b: -0.5198, loss: 0.0669,  grad a: -0.1150, grad b: 0.0607\n",
      "Itération 62:\n",
      "a: 1.1014, b: -0.5258, loss: 0.0652,  grad a: -0.1136, grad b: 0.0599\n",
      "Itération 63:\n",
      "a: 1.1128, b: -0.5318, loss: 0.0636,  grad a: -0.1121, grad b: 0.0592\n",
      "Itération 64:\n",
      "a: 1.1240, b: -0.5377, loss: 0.0620,  grad a: -0.1107, grad b: 0.0584\n",
      "Itération 65:\n",
      "a: 1.1351, b: -0.5436, loss: 0.0604,  grad a: -0.1093, grad b: 0.0577\n",
      "Itération 66:\n",
      "a: 1.1460, b: -0.5494, loss: 0.0589,  grad a: -0.1079, grad b: 0.0570\n",
      "Itération 67:\n",
      "a: 1.1568, b: -0.5551, loss: 0.0574,  grad a: -0.1066, grad b: 0.0562\n",
      "Itération 68:\n",
      "a: 1.1675, b: -0.5607, loss: 0.0560,  grad a: -0.1052, grad b: 0.0555\n",
      "Itération 69:\n",
      "a: 1.1780, b: -0.5662, loss: 0.0546,  grad a: -0.1039, grad b: 0.0548\n",
      "Itération 70:\n",
      "a: 1.1884, b: -0.5717, loss: 0.0532,  grad a: -0.1026, grad b: 0.0541\n",
      "Itération 71:\n",
      "a: 1.1986, b: -0.5771, loss: 0.0519,  grad a: -0.1013, grad b: 0.0534\n",
      "Itération 72:\n",
      "a: 1.2088, b: -0.5825, loss: 0.0506,  grad a: -0.1000, grad b: 0.0528\n",
      "Itération 73:\n",
      "a: 1.2188, b: -0.5877, loss: 0.0493,  grad a: -0.0987, grad b: 0.0521\n",
      "Itération 74:\n",
      "a: 1.2286, b: -0.5930, loss: 0.0481,  grad a: -0.0975, grad b: 0.0514\n",
      "Itération 75:\n",
      "a: 1.2384, b: -0.5981, loss: 0.0469,  grad a: -0.0963, grad b: 0.0508\n",
      "Itération 76:\n",
      "a: 1.2480, b: -0.6032, loss: 0.0457,  grad a: -0.0950, grad b: 0.0502\n",
      "Itération 77:\n",
      "a: 1.2575, b: -0.6082, loss: 0.0445,  grad a: -0.0938, grad b: 0.0495\n",
      "Itération 78:\n",
      "a: 1.2669, b: -0.6131, loss: 0.0434,  grad a: -0.0927, grad b: 0.0489\n",
      "Itération 79:\n",
      "a: 1.2762, b: -0.6180, loss: 0.0423,  grad a: -0.0915, grad b: 0.0483\n",
      "Itération 80:\n",
      "a: 1.2853, b: -0.6229, loss: 0.0413,  grad a: -0.0903, grad b: 0.0477\n",
      "Itération 81:\n",
      "a: 1.2944, b: -0.6276, loss: 0.0402,  grad a: -0.0892, grad b: 0.0471\n",
      "Itération 82:\n",
      "a: 1.3033, b: -0.6323, loss: 0.0392,  grad a: -0.0881, grad b: 0.0465\n",
      "Itération 83:\n",
      "a: 1.3121, b: -0.6370, loss: 0.0382,  grad a: -0.0869, grad b: 0.0459\n",
      "Itération 84:\n",
      "a: 1.3208, b: -0.6416, loss: 0.0373,  grad a: -0.0858, grad b: 0.0453\n",
      "Itération 85:\n",
      "a: 1.3294, b: -0.6461, loss: 0.0363,  grad a: -0.0848, grad b: 0.0447\n",
      "Itération 86:\n",
      "a: 1.3378, b: -0.6506, loss: 0.0354,  grad a: -0.0837, grad b: 0.0442\n",
      "Itération 87:\n",
      "a: 1.3462, b: -0.6550, loss: 0.0345,  grad a: -0.0826, grad b: 0.0436\n",
      "Itération 88:\n",
      "a: 1.3545, b: -0.6594, loss: 0.0337,  grad a: -0.0816, grad b: 0.0431\n",
      "Itération 89:\n",
      "a: 1.3626, b: -0.6637, loss: 0.0328,  grad a: -0.0806, grad b: 0.0425\n",
      "Itération 90:\n",
      "a: 1.3707, b: -0.6679, loss: 0.0320,  grad a: -0.0795, grad b: 0.0420\n",
      "Itération 91:\n",
      "a: 1.3786, b: -0.6721, loss: 0.0312,  grad a: -0.0785, grad b: 0.0414\n",
      "Itération 92:\n",
      "a: 1.3865, b: -0.6763, loss: 0.0304,  grad a: -0.0775, grad b: 0.0409\n",
      "Itération 93:\n",
      "a: 1.3942, b: -0.6803, loss: 0.0296,  grad a: -0.0766, grad b: 0.0404\n",
      "Itération 94:\n",
      "a: 1.4019, b: -0.6844, loss: 0.0289,  grad a: -0.0756, grad b: 0.0399\n",
      "Itération 95:\n",
      "a: 1.4095, b: -0.6884, loss: 0.0282,  grad a: -0.0746, grad b: 0.0394\n",
      "Itération 96:\n",
      "a: 1.4169, b: -0.6923, loss: 0.0275,  grad a: -0.0737, grad b: 0.0389\n",
      "Itération 97:\n",
      "a: 1.4243, b: -0.6962, loss: 0.0268,  grad a: -0.0728, grad b: 0.0384\n",
      "Itération 98:\n",
      "a: 1.4316, b: -0.7000, loss: 0.0261,  grad a: -0.0718, grad b: 0.0379\n",
      "Itération 99:\n",
      "a: 1.4388, b: -0.7038, loss: 0.0254,  grad a: -0.0709, grad b: 0.0374\n",
      "Itération 100:\n",
      "a: 1.4458, b: -0.7076, loss: 0.0248,  grad a: -0.0700, grad b: 0.0370\n",
      "Itération 101:\n",
      "a: 1.4528, b: -0.7113, loss: 0.0242,  grad a: -0.0692, grad b: 0.0365\n",
      "Itération 102:\n",
      "a: 1.4598, b: -0.7149, loss: 0.0236,  grad a: -0.0683, grad b: 0.0360\n",
      "Itération 103:\n",
      "a: 1.4666, b: -0.7185, loss: 0.0230,  grad a: -0.0674, grad b: 0.0356\n",
      "Itération 104:\n",
      "a: 1.4733, b: -0.7221, loss: 0.0224,  grad a: -0.0666, grad b: 0.0351\n",
      "Itération 105:\n",
      "a: 1.4800, b: -0.7256, loss: 0.0218,  grad a: -0.0657, grad b: 0.0347\n",
      "Itération 106:\n",
      "a: 1.4866, b: -0.7291, loss: 0.0213,  grad a: -0.0649, grad b: 0.0342\n",
      "Itération 107:\n",
      "a: 1.4931, b: -0.7325, loss: 0.0208,  grad a: -0.0641, grad b: 0.0338\n",
      "Itération 108:\n",
      "a: 1.4995, b: -0.7359, loss: 0.0202,  grad a: -0.0633, grad b: 0.0334\n",
      "Itération 109:\n",
      "a: 1.5058, b: -0.7392, loss: 0.0197,  grad a: -0.0625, grad b: 0.0330\n",
      "Itération 110:\n",
      "a: 1.5120, b: -0.7425, loss: 0.0192,  grad a: -0.0617, grad b: 0.0325\n",
      "Itération 111:\n",
      "a: 1.5182, b: -0.7458, loss: 0.0188,  grad a: -0.0609, grad b: 0.0321\n",
      "Itération 112:\n",
      "a: 1.5243, b: -0.7490, loss: 0.0183,  grad a: -0.0601, grad b: 0.0317\n",
      "Itération 113:\n",
      "a: 1.5303, b: -0.7521, loss: 0.0178,  grad a: -0.0594, grad b: 0.0313\n",
      "Itération 114:\n",
      "a: 1.5362, b: -0.7553, loss: 0.0174,  grad a: -0.0586, grad b: 0.0309\n",
      "Itération 115:\n",
      "a: 1.5421, b: -0.7584, loss: 0.0169,  grad a: -0.0579, grad b: 0.0305\n",
      "Itération 116:\n",
      "a: 1.5479, b: -0.7614, loss: 0.0165,  grad a: -0.0571, grad b: 0.0302\n",
      "Itération 117:\n",
      "a: 1.5536, b: -0.7644, loss: 0.0161,  grad a: -0.0564, grad b: 0.0298\n",
      "Itération 118:\n",
      "a: 1.5592, b: -0.7674, loss: 0.0157,  grad a: -0.0557, grad b: 0.0294\n",
      "Itération 119:\n",
      "a: 1.5648, b: -0.7704, loss: 0.0153,  grad a: -0.0550, grad b: 0.0290\n",
      "Itération 120:\n",
      "a: 1.5703, b: -0.7733, loss: 0.0149,  grad a: -0.0543, grad b: 0.0287\n",
      "Itération 121:\n",
      "a: 1.5757, b: -0.7761, loss: 0.0145,  grad a: -0.0536, grad b: 0.0283\n",
      "Itération 122:\n",
      "a: 1.5811, b: -0.7790, loss: 0.0142,  grad a: -0.0529, grad b: 0.0279\n",
      "Itération 123:\n",
      "a: 1.5864, b: -0.7817, loss: 0.0138,  grad a: -0.0523, grad b: 0.0276\n",
      "Itération 124:\n",
      "a: 1.5916, b: -0.7845, loss: 0.0135,  grad a: -0.0516, grad b: 0.0272\n",
      "Itération 125:\n",
      "a: 1.5968, b: -0.7872, loss: 0.0131,  grad a: -0.0510, grad b: 0.0269\n",
      "Itération 126:\n",
      "a: 1.6019, b: -0.7899, loss: 0.0128,  grad a: -0.0503, grad b: 0.0266\n",
      "Itération 127:\n",
      "a: 1.6069, b: -0.7926, loss: 0.0125,  grad a: -0.0497, grad b: 0.0262\n",
      "Itération 128:\n",
      "a: 1.6119, b: -0.7952, loss: 0.0122,  grad a: -0.0491, grad b: 0.0259\n",
      "Itération 129:\n",
      "a: 1.6168, b: -0.7978, loss: 0.0119,  grad a: -0.0484, grad b: 0.0256\n",
      "Itération 130:\n",
      "a: 1.6216, b: -0.8003, loss: 0.0116,  grad a: -0.0478, grad b: 0.0252\n",
      "Itération 131:\n",
      "a: 1.6264, b: -0.8029, loss: 0.0113,  grad a: -0.0472, grad b: 0.0249\n",
      "Itération 132:\n",
      "a: 1.6311, b: -0.8054, loss: 0.0110,  grad a: -0.0466, grad b: 0.0246\n",
      "Itération 133:\n",
      "a: 1.6358, b: -0.8078, loss: 0.0107,  grad a: -0.0460, grad b: 0.0243\n",
      "Itération 134:\n",
      "a: 1.6404, b: -0.8102, loss: 0.0104,  grad a: -0.0454, grad b: 0.0240\n",
      "Itération 135:\n",
      "a: 1.6449, b: -0.8126, loss: 0.0102,  grad a: -0.0449, grad b: 0.0237\n",
      "Itération 136:\n",
      "a: 1.6494, b: -0.8150, loss: 0.0099,  grad a: -0.0443, grad b: 0.0234\n",
      "Itération 137:\n",
      "a: 1.6539, b: -0.8173, loss: 0.0097,  grad a: -0.0437, grad b: 0.0231\n",
      "Itération 138:\n",
      "a: 1.6582, b: -0.8197, loss: 0.0094,  grad a: -0.0432, grad b: 0.0228\n",
      "Itération 139:\n",
      "a: 1.6626, b: -0.8219, loss: 0.0092,  grad a: -0.0426, grad b: 0.0225\n",
      "Itération 140:\n",
      "a: 1.6668, b: -0.8242, loss: 0.0090,  grad a: -0.0421, grad b: 0.0222\n",
      "Itération 141:\n",
      "a: 1.6710, b: -0.8264, loss: 0.0087,  grad a: -0.0416, grad b: 0.0219\n",
      "Itération 142:\n",
      "a: 1.6752, b: -0.8286, loss: 0.0085,  grad a: -0.0411, grad b: 0.0217\n",
      "Itération 143:\n",
      "a: 1.6793, b: -0.8308, loss: 0.0083,  grad a: -0.0405, grad b: 0.0214\n",
      "Itération 144:\n",
      "a: 1.6834, b: -0.8329, loss: 0.0081,  grad a: -0.0400, grad b: 0.0211\n",
      "Itération 145:\n",
      "a: 1.6874, b: -0.8350, loss: 0.0079,  grad a: -0.0395, grad b: 0.0209\n",
      "Itération 146:\n",
      "a: 1.6913, b: -0.8371, loss: 0.0077,  grad a: -0.0390, grad b: 0.0206\n",
      "Itération 147:\n",
      "a: 1.6952, b: -0.8392, loss: 0.0075,  grad a: -0.0385, grad b: 0.0203\n",
      "Itération 148:\n",
      "a: 1.6991, b: -0.8412, loss: 0.0073,  grad a: -0.0380, grad b: 0.0201\n",
      "Itération 149:\n",
      "a: 1.7029, b: -0.8432, loss: 0.0071,  grad a: -0.0376, grad b: 0.0198\n",
      "Itération 150:\n",
      "a: 1.7066, b: -0.8452, loss: 0.0070,  grad a: -0.0371, grad b: 0.0196\n",
      "Itération 151:\n",
      "a: 1.7103, b: -0.8471, loss: 0.0068,  grad a: -0.0366, grad b: 0.0193\n",
      "Itération 152:\n",
      "a: 1.7140, b: -0.8491, loss: 0.0066,  grad a: -0.0361, grad b: 0.0191\n",
      "Itération 153:\n",
      "a: 1.7176, b: -0.8510, loss: 0.0064,  grad a: -0.0357, grad b: 0.0188\n",
      "Itération 154:\n",
      "a: 1.7212, b: -0.8529, loss: 0.0063,  grad a: -0.0352, grad b: 0.0186\n",
      "Itération 155:\n",
      "a: 1.7247, b: -0.8547, loss: 0.0061,  grad a: -0.0348, grad b: 0.0184\n",
      "Itération 156:\n",
      "a: 1.7282, b: -0.8566, loss: 0.0060,  grad a: -0.0344, grad b: 0.0181\n",
      "Itération 157:\n",
      "a: 1.7316, b: -0.8584, loss: 0.0058,  grad a: -0.0339, grad b: 0.0179\n",
      "Itération 158:\n",
      "a: 1.7350, b: -0.8602, loss: 0.0057,  grad a: -0.0335, grad b: 0.0177\n",
      "Itération 159:\n",
      "a: 1.7384, b: -0.8619, loss: 0.0055,  grad a: -0.0331, grad b: 0.0175\n",
      "Itération 160:\n",
      "a: 1.7417, b: -0.8637, loss: 0.0054,  grad a: -0.0327, grad b: 0.0172\n",
      "Itération 161:\n",
      "a: 1.7449, b: -0.8654, loss: 0.0053,  grad a: -0.0322, grad b: 0.0170\n",
      "Itération 162:\n",
      "a: 1.7482, b: -0.8671, loss: 0.0051,  grad a: -0.0318, grad b: 0.0168\n",
      "Itération 163:\n",
      "a: 1.7513, b: -0.8688, loss: 0.0050,  grad a: -0.0314, grad b: 0.0166\n",
      "Itération 164:\n",
      "a: 1.7545, b: -0.8704, loss: 0.0049,  grad a: -0.0310, grad b: 0.0164\n",
      "Itération 165:\n",
      "a: 1.7576, b: -0.8721, loss: 0.0047,  grad a: -0.0306, grad b: 0.0162\n",
      "Itération 166:\n",
      "a: 1.7606, b: -0.8737, loss: 0.0046,  grad a: -0.0303, grad b: 0.0160\n",
      "Itération 167:\n",
      "a: 1.7637, b: -0.8753, loss: 0.0045,  grad a: -0.0299, grad b: 0.0158\n",
      "Itération 168:\n",
      "a: 1.7667, b: -0.8769, loss: 0.0044,  grad a: -0.0295, grad b: 0.0156\n",
      "Itération 169:\n",
      "a: 1.7696, b: -0.8784, loss: 0.0043,  grad a: -0.0291, grad b: 0.0154\n",
      "Itération 170:\n",
      "a: 1.7725, b: -0.8800, loss: 0.0042,  grad a: -0.0288, grad b: 0.0152\n",
      "Itération 171:\n",
      "a: 1.7754, b: -0.8815, loss: 0.0041,  grad a: -0.0284, grad b: 0.0150\n",
      "Itération 172:\n",
      "a: 1.7782, b: -0.8830, loss: 0.0040,  grad a: -0.0280, grad b: 0.0148\n",
      "Itération 173:\n",
      "a: 1.7810, b: -0.8845, loss: 0.0039,  grad a: -0.0277, grad b: 0.0146\n",
      "Itération 174:\n",
      "a: 1.7838, b: -0.8859, loss: 0.0038,  grad a: -0.0273, grad b: 0.0144\n",
      "Itération 175:\n",
      "a: 1.7865, b: -0.8874, loss: 0.0037,  grad a: -0.0270, grad b: 0.0142\n",
      "Itération 176:\n",
      "a: 1.7892, b: -0.8888, loss: 0.0036,  grad a: -0.0266, grad b: 0.0141\n",
      "Itération 177:\n",
      "a: 1.7919, b: -0.8902, loss: 0.0035,  grad a: -0.0263, grad b: 0.0139\n",
      "Itération 178:\n",
      "a: 1.7945, b: -0.8916, loss: 0.0034,  grad a: -0.0260, grad b: 0.0137\n",
      "Itération 179:\n",
      "a: 1.7971, b: -0.8929, loss: 0.0033,  grad a: -0.0256, grad b: 0.0135\n",
      "Itération 180:\n",
      "a: 1.7997, b: -0.8943, loss: 0.0032,  grad a: -0.0253, grad b: 0.0134\n",
      "Itération 181:\n",
      "a: 1.8022, b: -0.8956, loss: 0.0032,  grad a: -0.0250, grad b: 0.0132\n",
      "Itération 182:\n",
      "a: 1.8047, b: -0.8970, loss: 0.0031,  grad a: -0.0247, grad b: 0.0130\n",
      "Itération 183:\n",
      "a: 1.8072, b: -0.8983, loss: 0.0030,  grad a: -0.0244, grad b: 0.0129\n",
      "Itération 184:\n",
      "a: 1.8096, b: -0.8995, loss: 0.0029,  grad a: -0.0241, grad b: 0.0127\n",
      "Itération 185:\n",
      "a: 1.8120, b: -0.9008, loss: 0.0029,  grad a: -0.0238, grad b: 0.0125\n",
      "Itération 186:\n",
      "a: 1.8144, b: -0.9021, loss: 0.0028,  grad a: -0.0235, grad b: 0.0124\n",
      "Itération 187:\n",
      "a: 1.8168, b: -0.9033, loss: 0.0027,  grad a: -0.0232, grad b: 0.0122\n",
      "Itération 188:\n",
      "a: 1.8191, b: -0.9045, loss: 0.0026,  grad a: -0.0229, grad b: 0.0121\n",
      "Itération 189:\n",
      "a: 1.8214, b: -0.9057, loss: 0.0026,  grad a: -0.0226, grad b: 0.0119\n",
      "Itération 190:\n",
      "a: 1.8236, b: -0.9069, loss: 0.0025,  grad a: -0.0223, grad b: 0.0118\n",
      "Itération 191:\n",
      "a: 1.8258, b: -0.9081, loss: 0.0025,  grad a: -0.0220, grad b: 0.0116\n",
      "Itération 192:\n",
      "a: 1.8280, b: -0.9093, loss: 0.0024,  grad a: -0.0217, grad b: 0.0115\n",
      "Itération 193:\n",
      "a: 1.8302, b: -0.9104, loss: 0.0023,  grad a: -0.0215, grad b: 0.0113\n",
      "Itération 194:\n",
      "a: 1.8324, b: -0.9115, loss: 0.0023,  grad a: -0.0212, grad b: 0.0112\n",
      "Itération 195:\n",
      "a: 1.8345, b: -0.9127, loss: 0.0022,  grad a: -0.0209, grad b: 0.0110\n",
      "Itération 196:\n",
      "a: 1.8366, b: -0.9138, loss: 0.0022,  grad a: -0.0207, grad b: 0.0109\n",
      "Itération 197:\n",
      "a: 1.8386, b: -0.9149, loss: 0.0021,  grad a: -0.0204, grad b: 0.0108\n",
      "Itération 198:\n",
      "a: 1.8407, b: -0.9159, loss: 0.0021,  grad a: -0.0201, grad b: 0.0106\n",
      "Itération 199:\n",
      "a: 1.8427, b: -0.9170, loss: 0.0020,  grad a: -0.0199, grad b: 0.0105\n",
      "Itération 200:\n",
      "a: 1.8447, b: -0.9180, loss: 0.0019,  grad a: -0.0196, grad b: 0.0104\n",
      "Itération 201:\n",
      "a: 1.8466, b: -0.9191, loss: 0.0019,  grad a: -0.0194, grad b: 0.0102\n",
      "Itération 202:\n",
      "a: 1.8486, b: -0.9201, loss: 0.0019,  grad a: -0.0191, grad b: 0.0101\n",
      "Itération 203:\n",
      "a: 1.8505, b: -0.9211, loss: 0.0018,  grad a: -0.0189, grad b: 0.0100\n",
      "Itération 204:\n",
      "a: 1.8524, b: -0.9221, loss: 0.0018,  grad a: -0.0187, grad b: 0.0098\n",
      "Itération 205:\n",
      "a: 1.8543, b: -0.9231, loss: 0.0017,  grad a: -0.0184, grad b: 0.0097\n",
      "Itération 206:\n",
      "a: 1.8561, b: -0.9241, loss: 0.0017,  grad a: -0.0182, grad b: 0.0096\n",
      "Itération 207:\n",
      "a: 1.8579, b: -0.9250, loss: 0.0016,  grad a: -0.0180, grad b: 0.0095\n",
      "Itération 208:\n",
      "a: 1.8597, b: -0.9260, loss: 0.0016,  grad a: -0.0177, grad b: 0.0094\n",
      "Itération 209:\n",
      "a: 1.8615, b: -0.9269, loss: 0.0016,  grad a: -0.0175, grad b: 0.0092\n",
      "Itération 210:\n",
      "a: 1.8632, b: -0.9278, loss: 0.0015,  grad a: -0.0173, grad b: 0.0091\n",
      "Itération 211:\n",
      "a: 1.8650, b: -0.9287, loss: 0.0015,  grad a: -0.0171, grad b: 0.0090\n",
      "Itération 212:\n",
      "a: 1.8667, b: -0.9296, loss: 0.0014,  grad a: -0.0169, grad b: 0.0089\n",
      "Itération 213:\n",
      "a: 1.8684, b: -0.9305, loss: 0.0014,  grad a: -0.0166, grad b: 0.0088\n",
      "Itération 214:\n",
      "a: 1.8700, b: -0.9314, loss: 0.0014,  grad a: -0.0164, grad b: 0.0087\n",
      "Itération 215:\n",
      "a: 1.8717, b: -0.9323, loss: 0.0013,  grad a: -0.0162, grad b: 0.0086\n",
      "Itération 216:\n",
      "a: 1.8733, b: -0.9331, loss: 0.0013,  grad a: -0.0160, grad b: 0.0085\n",
      "Itération 217:\n",
      "a: 1.8749, b: -0.9340, loss: 0.0013,  grad a: -0.0158, grad b: 0.0083\n",
      "Itération 218:\n",
      "a: 1.8765, b: -0.9348, loss: 0.0012,  grad a: -0.0156, grad b: 0.0082\n",
      "Itération 219:\n",
      "a: 1.8780, b: -0.9356, loss: 0.0012,  grad a: -0.0154, grad b: 0.0081\n",
      "Itération 220:\n",
      "a: 1.8796, b: -0.9364, loss: 0.0012,  grad a: -0.0152, grad b: 0.0080\n",
      "Itération 221:\n",
      "a: 1.8811, b: -0.9373, loss: 0.0011,  grad a: -0.0150, grad b: 0.0079\n",
      "Itération 222:\n",
      "a: 1.8826, b: -0.9380, loss: 0.0011,  grad a: -0.0148, grad b: 0.0078\n",
      "Itération 223:\n",
      "a: 1.8841, b: -0.9388, loss: 0.0011,  grad a: -0.0147, grad b: 0.0077\n",
      "Itération 224:\n",
      "a: 1.8855, b: -0.9396, loss: 0.0011,  grad a: -0.0145, grad b: 0.0076\n",
      "Itération 225:\n",
      "a: 1.8870, b: -0.9404, loss: 0.0010,  grad a: -0.0143, grad b: 0.0075\n",
      "Itération 226:\n",
      "a: 1.8884, b: -0.9411, loss: 0.0010,  grad a: -0.0141, grad b: 0.0074\n",
      "Itération 227:\n",
      "a: 1.8898, b: -0.9419, loss: 0.0010,  grad a: -0.0139, grad b: 0.0073\n",
      "Itération 228:\n",
      "a: 1.8912, b: -0.9426, loss: 0.0010,  grad a: -0.0137, grad b: 0.0073\n",
      "Itération 229:\n",
      "a: 1.8926, b: -0.9433, loss: 0.0009,  grad a: -0.0136, grad b: 0.0072\n",
      "Itération 230:\n",
      "a: 1.8940, b: -0.9440, loss: 0.0009,  grad a: -0.0134, grad b: 0.0071\n",
      "Itération 231:\n",
      "a: 1.8953, b: -0.9447, loss: 0.0009,  grad a: -0.0132, grad b: 0.0070\n",
      "Itération 232:\n",
      "a: 1.8966, b: -0.9454, loss: 0.0009,  grad a: -0.0131, grad b: 0.0069\n",
      "Itération 233:\n",
      "a: 1.8979, b: -0.9461, loss: 0.0008,  grad a: -0.0129, grad b: 0.0068\n",
      "Itération 234:\n",
      "a: 1.8992, b: -0.9468, loss: 0.0008,  grad a: -0.0127, grad b: 0.0067\n",
      "Itération 235:\n",
      "a: 1.9005, b: -0.9475, loss: 0.0008,  grad a: -0.0126, grad b: 0.0066\n",
      "Itération 236:\n",
      "a: 1.9017, b: -0.9482, loss: 0.0008,  grad a: -0.0124, grad b: 0.0066\n",
      "Itération 237:\n",
      "a: 1.9030, b: -0.9488, loss: 0.0008,  grad a: -0.0123, grad b: 0.0065\n",
      "Itération 238:\n",
      "a: 1.9042, b: -0.9495, loss: 0.0007,  grad a: -0.0121, grad b: 0.0064\n",
      "Itération 239:\n",
      "a: 1.9054, b: -0.9501, loss: 0.0007,  grad a: -0.0120, grad b: 0.0063\n",
      "Itération 240:\n",
      "a: 1.9066, b: -0.9507, loss: 0.0007,  grad a: -0.0118, grad b: 0.0062\n",
      "Itération 241:\n",
      "a: 1.9078, b: -0.9513, loss: 0.0007,  grad a: -0.0117, grad b: 0.0061\n",
      "Itération 242:\n",
      "a: 1.9090, b: -0.9520, loss: 0.0007,  grad a: -0.0115, grad b: 0.0061\n",
      "Itération 243:\n",
      "a: 1.9101, b: -0.9526, loss: 0.0007,  grad a: -0.0114, grad b: 0.0060\n",
      "Itération 244:\n",
      "a: 1.9113, b: -0.9532, loss: 0.0006,  grad a: -0.0112, grad b: 0.0059\n",
      "Itération 245:\n",
      "a: 1.9124, b: -0.9538, loss: 0.0006,  grad a: -0.0111, grad b: 0.0058\n",
      "Itération 246:\n",
      "a: 1.9135, b: -0.9543, loss: 0.0006,  grad a: -0.0109, grad b: 0.0058\n",
      "Itération 247:\n",
      "a: 1.9146, b: -0.9549, loss: 0.0006,  grad a: -0.0108, grad b: 0.0057\n",
      "Itération 248:\n",
      "a: 1.9157, b: -0.9555, loss: 0.0006,  grad a: -0.0107, grad b: 0.0056\n",
      "Itération 249:\n",
      "a: 1.9167, b: -0.9561, loss: 0.0006,  grad a: -0.0105, grad b: 0.0056\n",
      "Itération 250:\n",
      "a: 1.9178, b: -0.9566, loss: 0.0005,  grad a: -0.0104, grad b: 0.0055\n",
      "Itération 251:\n",
      "a: 1.9188, b: -0.9572, loss: 0.0005,  grad a: -0.0103, grad b: 0.0054\n",
      "Itération 252:\n",
      "a: 1.9198, b: -0.9577, loss: 0.0005,  grad a: -0.0101, grad b: 0.0053\n",
      "Itération 253:\n",
      "a: 1.9208, b: -0.9582, loss: 0.0005,  grad a: -0.0100, grad b: 0.0053\n",
      "Itération 254:\n",
      "a: 1.9219, b: -0.9588, loss: 0.0005,  grad a: -0.0099, grad b: 0.0052\n",
      "Itération 255:\n",
      "a: 1.9228, b: -0.9593, loss: 0.0005,  grad a: -0.0098, grad b: 0.0051\n",
      "Itération 256:\n",
      "a: 1.9238, b: -0.9598, loss: 0.0005,  grad a: -0.0096, grad b: 0.0051\n",
      "Itération 257:\n",
      "a: 1.9248, b: -0.9603, loss: 0.0005,  grad a: -0.0095, grad b: 0.0050\n",
      "Itération 258:\n",
      "a: 1.9257, b: -0.9608, loss: 0.0004,  grad a: -0.0094, grad b: 0.0050\n",
      "Itération 259:\n",
      "a: 1.9267, b: -0.9613, loss: 0.0004,  grad a: -0.0093, grad b: 0.0049\n",
      "Itération 260:\n",
      "a: 1.9276, b: -0.9618, loss: 0.0004,  grad a: -0.0092, grad b: 0.0048\n",
      "Itération 261:\n",
      "a: 1.9285, b: -0.9623, loss: 0.0004,  grad a: -0.0090, grad b: 0.0048\n",
      "Itération 262:\n",
      "a: 1.9294, b: -0.9628, loss: 0.0004,  grad a: -0.0089, grad b: 0.0047\n",
      "Itération 263:\n",
      "a: 1.9303, b: -0.9632, loss: 0.0004,  grad a: -0.0088, grad b: 0.0046\n",
      "Itération 264:\n",
      "a: 1.9312, b: -0.9637, loss: 0.0004,  grad a: -0.0087, grad b: 0.0046\n",
      "Itération 265:\n",
      "a: 1.9321, b: -0.9641, loss: 0.0004,  grad a: -0.0086, grad b: 0.0045\n",
      "Itération 266:\n",
      "a: 1.9329, b: -0.9646, loss: 0.0004,  grad a: -0.0085, grad b: 0.0045\n",
      "Itération 267:\n",
      "a: 1.9338, b: -0.9650, loss: 0.0004,  grad a: -0.0084, grad b: 0.0044\n",
      "Itération 268:\n",
      "a: 1.9346, b: -0.9655, loss: 0.0003,  grad a: -0.0083, grad b: 0.0044\n",
      "Itération 269:\n",
      "a: 1.9354, b: -0.9659, loss: 0.0003,  grad a: -0.0082, grad b: 0.0043\n",
      "Itération 270:\n",
      "a: 1.9362, b: -0.9664, loss: 0.0003,  grad a: -0.0081, grad b: 0.0043\n",
      "Itération 271:\n",
      "a: 1.9370, b: -0.9668, loss: 0.0003,  grad a: -0.0080, grad b: 0.0042\n",
      "Itération 272:\n",
      "a: 1.9378, b: -0.9672, loss: 0.0003,  grad a: -0.0079, grad b: 0.0041\n",
      "Itération 273:\n",
      "a: 1.9386, b: -0.9676, loss: 0.0003,  grad a: -0.0078, grad b: 0.0041\n",
      "Itération 274:\n",
      "a: 1.9394, b: -0.9680, loss: 0.0003,  grad a: -0.0077, grad b: 0.0040\n",
      "Itération 275:\n",
      "a: 1.9402, b: -0.9684, loss: 0.0003,  grad a: -0.0076, grad b: 0.0040\n",
      "Itération 276:\n",
      "a: 1.9409, b: -0.9688, loss: 0.0003,  grad a: -0.0075, grad b: 0.0039\n",
      "Itération 277:\n",
      "a: 1.9417, b: -0.9692, loss: 0.0003,  grad a: -0.0074, grad b: 0.0039\n",
      "Itération 278:\n",
      "a: 1.9424, b: -0.9696, loss: 0.0003,  grad a: -0.0073, grad b: 0.0038\n",
      "Itération 279:\n",
      "a: 1.9431, b: -0.9700, loss: 0.0003,  grad a: -0.0072, grad b: 0.0038\n",
      "Itération 280:\n",
      "a: 1.9439, b: -0.9704, loss: 0.0003,  grad a: -0.0071, grad b: 0.0037\n",
      "Itération 281:\n",
      "a: 1.9446, b: -0.9707, loss: 0.0002,  grad a: -0.0070, grad b: 0.0037\n",
      "Itération 282:\n",
      "a: 1.9453, b: -0.9711, loss: 0.0002,  grad a: -0.0069, grad b: 0.0037\n",
      "Itération 283:\n",
      "a: 1.9460, b: -0.9715, loss: 0.0002,  grad a: -0.0068, grad b: 0.0036\n",
      "Itération 284:\n",
      "a: 1.9466, b: -0.9718, loss: 0.0002,  grad a: -0.0067, grad b: 0.0036\n",
      "Itération 285:\n",
      "a: 1.9473, b: -0.9722, loss: 0.0002,  grad a: -0.0067, grad b: 0.0035\n",
      "Itération 286:\n",
      "a: 1.9480, b: -0.9726, loss: 0.0002,  grad a: -0.0066, grad b: 0.0035\n",
      "Itération 287:\n",
      "a: 1.9486, b: -0.9729, loss: 0.0002,  grad a: -0.0065, grad b: 0.0034\n",
      "Itération 288:\n",
      "a: 1.9493, b: -0.9732, loss: 0.0002,  grad a: -0.0064, grad b: 0.0034\n",
      "Itération 289:\n",
      "a: 1.9499, b: -0.9736, loss: 0.0002,  grad a: -0.0063, grad b: 0.0033\n",
      "Itération 290:\n",
      "a: 1.9506, b: -0.9739, loss: 0.0002,  grad a: -0.0062, grad b: 0.0033\n",
      "Itération 291:\n",
      "a: 1.9512, b: -0.9742, loss: 0.0002,  grad a: -0.0062, grad b: 0.0033\n",
      "Itération 292:\n",
      "a: 1.9518, b: -0.9746, loss: 0.0002,  grad a: -0.0061, grad b: 0.0032\n",
      "Itération 293:\n",
      "a: 1.9524, b: -0.9749, loss: 0.0002,  grad a: -0.0060, grad b: 0.0032\n",
      "Itération 294:\n",
      "a: 1.9530, b: -0.9752, loss: 0.0002,  grad a: -0.0059, grad b: 0.0031\n",
      "Itération 295:\n",
      "a: 1.9536, b: -0.9755, loss: 0.0002,  grad a: -0.0059, grad b: 0.0031\n",
      "Itération 296:\n",
      "a: 1.9542, b: -0.9758, loss: 0.0002,  grad a: -0.0058, grad b: 0.0031\n",
      "Itération 297:\n",
      "a: 1.9548, b: -0.9761, loss: 0.0002,  grad a: -0.0057, grad b: 0.0030\n",
      "Itération 298:\n",
      "a: 1.9553, b: -0.9764, loss: 0.0002,  grad a: -0.0056, grad b: 0.0030\n",
      "Itération 299:\n",
      "a: 1.9559, b: -0.9767, loss: 0.0002,  grad a: -0.0056, grad b: 0.0029\n",
      "Itération 300:\n",
      "a: 1.9565, b: -0.9770, loss: 0.0002,  grad a: -0.0055, grad b: 0.0029\n",
      "Itération 301:\n",
      "a: 1.9570, b: -0.9773, loss: 0.0001,  grad a: -0.0054, grad b: 0.0029\n",
      "Itération 302:\n",
      "a: 1.9576, b: -0.9776, loss: 0.0001,  grad a: -0.0054, grad b: 0.0028\n",
      "Itération 303:\n",
      "a: 1.9581, b: -0.9779, loss: 0.0001,  grad a: -0.0053, grad b: 0.0028\n",
      "Itération 304:\n",
      "a: 1.9586, b: -0.9782, loss: 0.0001,  grad a: -0.0052, grad b: 0.0028\n",
      "Itération 305:\n",
      "a: 1.9591, b: -0.9784, loss: 0.0001,  grad a: -0.0052, grad b: 0.0027\n",
      "Itération 306:\n",
      "a: 1.9597, b: -0.9787, loss: 0.0001,  grad a: -0.0051, grad b: 0.0027\n",
      "Itération 307:\n",
      "a: 1.9602, b: -0.9790, loss: 0.0001,  grad a: -0.0050, grad b: 0.0027\n",
      "Itération 308:\n",
      "a: 1.9607, b: -0.9793, loss: 0.0001,  grad a: -0.0050, grad b: 0.0026\n",
      "Itération 309:\n",
      "a: 1.9612, b: -0.9795, loss: 0.0001,  grad a: -0.0049, grad b: 0.0026\n",
      "Itération 310:\n",
      "a: 1.9617, b: -0.9798, loss: 0.0001,  grad a: -0.0048, grad b: 0.0026\n",
      "Itération 311:\n",
      "a: 1.9622, b: -0.9800, loss: 0.0001,  grad a: -0.0048, grad b: 0.0025\n",
      "Itération 312:\n",
      "a: 1.9626, b: -0.9803, loss: 0.0001,  grad a: -0.0047, grad b: 0.0025\n",
      "Itération 313:\n",
      "a: 1.9631, b: -0.9805, loss: 0.0001,  grad a: -0.0047, grad b: 0.0025\n",
      "Itération 314:\n",
      "a: 1.9636, b: -0.9808, loss: 0.0001,  grad a: -0.0046, grad b: 0.0024\n",
      "Itération 315:\n",
      "a: 1.9640, b: -0.9810, loss: 0.0001,  grad a: -0.0045, grad b: 0.0024\n",
      "Itération 316:\n",
      "a: 1.9645, b: -0.9813, loss: 0.0001,  grad a: -0.0045, grad b: 0.0024\n",
      "Itération 317:\n",
      "a: 1.9649, b: -0.9815, loss: 0.0001,  grad a: -0.0044, grad b: 0.0023\n",
      "Itération 318:\n",
      "a: 1.9654, b: -0.9817, loss: 0.0001,  grad a: -0.0044, grad b: 0.0023\n",
      "Itération 319:\n",
      "a: 1.9658, b: -0.9820, loss: 0.0001,  grad a: -0.0043, grad b: 0.0023\n",
      "Itération 320:\n",
      "a: 1.9662, b: -0.9822, loss: 0.0001,  grad a: -0.0043, grad b: 0.0023\n",
      "Itération 321:\n",
      "a: 1.9667, b: -0.9824, loss: 0.0001,  grad a: -0.0042, grad b: 0.0022\n",
      "Itération 322:\n",
      "a: 1.9671, b: -0.9826, loss: 0.0001,  grad a: -0.0042, grad b: 0.0022\n",
      "Itération 323:\n",
      "a: 1.9675, b: -0.9829, loss: 0.0001,  grad a: -0.0041, grad b: 0.0022\n",
      "Itération 324:\n",
      "a: 1.9679, b: -0.9831, loss: 0.0001,  grad a: -0.0041, grad b: 0.0021\n",
      "Itération 325:\n",
      "a: 1.9683, b: -0.9833, loss: 0.0001,  grad a: -0.0040, grad b: 0.0021\n",
      "Itération 326:\n",
      "a: 1.9687, b: -0.9835, loss: 0.0001,  grad a: -0.0040, grad b: 0.0021\n",
      "Itération 327:\n",
      "a: 1.9691, b: -0.9837, loss: 0.0001,  grad a: -0.0039, grad b: 0.0021\n",
      "Itération 328:\n",
      "a: 1.9695, b: -0.9839, loss: 0.0001,  grad a: -0.0039, grad b: 0.0020\n",
      "Itération 329:\n",
      "a: 1.9699, b: -0.9841, loss: 0.0001,  grad a: -0.0038, grad b: 0.0020\n",
      "Itération 330:\n",
      "a: 1.9703, b: -0.9843, loss: 0.0001,  grad a: -0.0038, grad b: 0.0020\n",
      "Itération 331:\n",
      "a: 1.9707, b: -0.9845, loss: 0.0001,  grad a: -0.0037, grad b: 0.0020\n",
      "Itération 332:\n",
      "a: 1.9710, b: -0.9847, loss: 0.0001,  grad a: -0.0037, grad b: 0.0019\n",
      "Itération 333:\n",
      "a: 1.9714, b: -0.9849, loss: 0.0001,  grad a: -0.0036, grad b: 0.0019\n",
      "Itération 334:\n",
      "a: 1.9718, b: -0.9851, loss: 0.0001,  grad a: -0.0036, grad b: 0.0019\n",
      "Itération 335:\n",
      "a: 1.9721, b: -0.9853, loss: 0.0001,  grad a: -0.0035, grad b: 0.0019\n",
      "Itération 336:\n",
      "a: 1.9725, b: -0.9855, loss: 0.0001,  grad a: -0.0035, grad b: 0.0018\n",
      "Itération 337:\n",
      "a: 1.9728, b: -0.9857, loss: 0.0001,  grad a: -0.0034, grad b: 0.0018\n",
      "Itération 338:\n",
      "a: 1.9732, b: -0.9858, loss: 0.0001,  grad a: -0.0034, grad b: 0.0018\n",
      "Itération 339:\n",
      "a: 1.9735, b: -0.9860, loss: 0.0001,  grad a: -0.0034, grad b: 0.0018\n",
      "Itération 340:\n",
      "a: 1.9738, b: -0.9862, loss: 0.0001,  grad a: -0.0033, grad b: 0.0017\n",
      "Itération 341:\n",
      "a: 1.9742, b: -0.9864, loss: 0.0001,  grad a: -0.0033, grad b: 0.0017\n",
      "Itération 342:\n",
      "a: 1.9745, b: -0.9865, loss: 0.0001,  grad a: -0.0032, grad b: 0.0017\n",
      "Itération 343:\n",
      "a: 1.9748, b: -0.9867, loss: 0.0001,  grad a: -0.0032, grad b: 0.0017\n",
      "Itération 344:\n",
      "a: 1.9751, b: -0.9869, loss: 0.0000,  grad a: -0.0031, grad b: 0.0017\n",
      "Itération 345:\n",
      "a: 1.9754, b: -0.9870, loss: 0.0000,  grad a: -0.0031, grad b: 0.0016\n",
      "Itération 346:\n",
      "a: 1.9757, b: -0.9872, loss: 0.0000,  grad a: -0.0031, grad b: 0.0016\n",
      "Itération 347:\n",
      "a: 1.9761, b: -0.9874, loss: 0.0000,  grad a: -0.0030, grad b: 0.0016\n",
      "Itération 348:\n",
      "a: 1.9764, b: -0.9875, loss: 0.0000,  grad a: -0.0030, grad b: 0.0016\n",
      "Itération 349:\n",
      "a: 1.9767, b: -0.9877, loss: 0.0000,  grad a: -0.0030, grad b: 0.0016\n",
      "Itération 350:\n",
      "a: 1.9770, b: -0.9878, loss: 0.0000,  grad a: -0.0029, grad b: 0.0015\n",
      "Itération 351:\n",
      "a: 1.9772, b: -0.9880, loss: 0.0000,  grad a: -0.0029, grad b: 0.0015\n",
      "Itération 352:\n",
      "a: 1.9775, b: -0.9881, loss: 0.0000,  grad a: -0.0028, grad b: 0.0015\n",
      "Itération 353:\n",
      "a: 1.9778, b: -0.9883, loss: 0.0000,  grad a: -0.0028, grad b: 0.0015\n",
      "Itération 354:\n",
      "a: 1.9781, b: -0.9884, loss: 0.0000,  grad a: -0.0028, grad b: 0.0015\n",
      "Itération 355:\n",
      "a: 1.9784, b: -0.9886, loss: 0.0000,  grad a: -0.0027, grad b: 0.0014\n",
      "Itération 356:\n",
      "a: 1.9786, b: -0.9887, loss: 0.0000,  grad a: -0.0027, grad b: 0.0014\n",
      "Itération 357:\n",
      "a: 1.9789, b: -0.9889, loss: 0.0000,  grad a: -0.0027, grad b: 0.0014\n",
      "Itération 358:\n",
      "a: 1.9792, b: -0.9890, loss: 0.0000,  grad a: -0.0026, grad b: 0.0014\n",
      "Itération 359:\n",
      "a: 1.9794, b: -0.9892, loss: 0.0000,  grad a: -0.0026, grad b: 0.0014\n",
      "Itération 360:\n",
      "a: 1.9797, b: -0.9893, loss: 0.0000,  grad a: -0.0026, grad b: 0.0014\n",
      "Itération 361:\n",
      "a: 1.9800, b: -0.9894, loss: 0.0000,  grad a: -0.0025, grad b: 0.0013\n",
      "Itération 362:\n",
      "a: 1.9802, b: -0.9896, loss: 0.0000,  grad a: -0.0025, grad b: 0.0013\n",
      "Itération 363:\n",
      "a: 1.9805, b: -0.9897, loss: 0.0000,  grad a: -0.0025, grad b: 0.0013\n",
      "Itération 364:\n",
      "a: 1.9807, b: -0.9898, loss: 0.0000,  grad a: -0.0024, grad b: 0.0013\n",
      "Itération 365:\n",
      "a: 1.9810, b: -0.9900, loss: 0.0000,  grad a: -0.0024, grad b: 0.0013\n",
      "Itération 366:\n",
      "a: 1.9812, b: -0.9901, loss: 0.0000,  grad a: -0.0024, grad b: 0.0013\n",
      "Itération 367:\n",
      "a: 1.9814, b: -0.9902, loss: 0.0000,  grad a: -0.0023, grad b: 0.0012\n",
      "Itération 368:\n",
      "a: 1.9817, b: -0.9903, loss: 0.0000,  grad a: -0.0023, grad b: 0.0012\n",
      "Itération 369:\n",
      "a: 1.9819, b: -0.9904, loss: 0.0000,  grad a: -0.0023, grad b: 0.0012\n",
      "Itération 370:\n",
      "a: 1.9821, b: -0.9906, loss: 0.0000,  grad a: -0.0023, grad b: 0.0012\n",
      "Itération 371:\n",
      "a: 1.9824, b: -0.9907, loss: 0.0000,  grad a: -0.0022, grad b: 0.0012\n",
      "Itération 372:\n",
      "a: 1.9826, b: -0.9908, loss: 0.0000,  grad a: -0.0022, grad b: 0.0012\n",
      "Itération 373:\n",
      "a: 1.9828, b: -0.9909, loss: 0.0000,  grad a: -0.0022, grad b: 0.0011\n",
      "Itération 374:\n",
      "a: 1.9830, b: -0.9910, loss: 0.0000,  grad a: -0.0021, grad b: 0.0011\n",
      "Itération 375:\n",
      "a: 1.9832, b: -0.9912, loss: 0.0000,  grad a: -0.0021, grad b: 0.0011\n",
      "Itération 376:\n",
      "a: 1.9834, b: -0.9913, loss: 0.0000,  grad a: -0.0021, grad b: 0.0011\n",
      "Itération 377:\n",
      "a: 1.9837, b: -0.9914, loss: 0.0000,  grad a: -0.0021, grad b: 0.0011\n",
      "Itération 378:\n",
      "a: 1.9839, b: -0.9915, loss: 0.0000,  grad a: -0.0020, grad b: 0.0011\n",
      "Itération 379:\n",
      "a: 1.9841, b: -0.9916, loss: 0.0000,  grad a: -0.0020, grad b: 0.0011\n",
      "Itération 380:\n",
      "a: 1.9843, b: -0.9917, loss: 0.0000,  grad a: -0.0020, grad b: 0.0010\n",
      "Itération 381:\n",
      "a: 1.9845, b: -0.9918, loss: 0.0000,  grad a: -0.0020, grad b: 0.0010\n",
      "Itération 382:\n",
      "a: 1.9847, b: -0.9919, loss: 0.0000,  grad a: -0.0019, grad b: 0.0010\n",
      "Itération 383:\n",
      "a: 1.9849, b: -0.9920, loss: 0.0000,  grad a: -0.0019, grad b: 0.0010\n",
      "Itération 384:\n",
      "a: 1.9850, b: -0.9921, loss: 0.0000,  grad a: -0.0019, grad b: 0.0010\n",
      "Itération 385:\n",
      "a: 1.9852, b: -0.9922, loss: 0.0000,  grad a: -0.0019, grad b: 0.0010\n",
      "Itération 386:\n",
      "a: 1.9854, b: -0.9923, loss: 0.0000,  grad a: -0.0018, grad b: 0.0010\n",
      "Itération 387:\n",
      "a: 1.9856, b: -0.9924, loss: 0.0000,  grad a: -0.0018, grad b: 0.0010\n",
      "Itération 388:\n",
      "a: 1.9858, b: -0.9925, loss: 0.0000,  grad a: -0.0018, grad b: 0.0009\n",
      "Itération 389:\n",
      "a: 1.9860, b: -0.9926, loss: 0.0000,  grad a: -0.0018, grad b: 0.0009\n",
      "Itération 390:\n",
      "a: 1.9861, b: -0.9927, loss: 0.0000,  grad a: -0.0018, grad b: 0.0009\n",
      "Itération 391:\n",
      "a: 1.9863, b: -0.9928, loss: 0.0000,  grad a: -0.0017, grad b: 0.0009\n",
      "Itération 392:\n",
      "a: 1.9865, b: -0.9929, loss: 0.0000,  grad a: -0.0017, grad b: 0.0009\n",
      "Itération 393:\n",
      "a: 1.9867, b: -0.9930, loss: 0.0000,  grad a: -0.0017, grad b: 0.0009\n",
      "Itération 394:\n",
      "a: 1.9868, b: -0.9931, loss: 0.0000,  grad a: -0.0017, grad b: 0.0009\n",
      "Itération 395:\n",
      "a: 1.9870, b: -0.9931, loss: 0.0000,  grad a: -0.0016, grad b: 0.0009\n",
      "Itération 396:\n",
      "a: 1.9872, b: -0.9932, loss: 0.0000,  grad a: -0.0016, grad b: 0.0009\n",
      "Itération 397:\n",
      "a: 1.9873, b: -0.9933, loss: 0.0000,  grad a: -0.0016, grad b: 0.0008\n",
      "Itération 398:\n",
      "a: 1.9875, b: -0.9934, loss: 0.0000,  grad a: -0.0016, grad b: 0.0008\n",
      "Itération 399:\n",
      "a: 1.9876, b: -0.9935, loss: 0.0000,  grad a: -0.0016, grad b: 0.0008\n",
      "Itération 400:\n",
      "a: 1.9878, b: -0.9936, loss: 0.0000,  grad a: -0.0015, grad b: 0.0008\n",
      "Itération 401:\n",
      "a: 1.9880, b: -0.9936, loss: 0.0000,  grad a: -0.0015, grad b: 0.0008\n",
      "Itération 402:\n",
      "a: 1.9881, b: -0.9937, loss: 0.0000,  grad a: -0.0015, grad b: 0.0008\n",
      "Itération 403:\n",
      "a: 1.9883, b: -0.9938, loss: 0.0000,  grad a: -0.0015, grad b: 0.0008\n",
      "Itération 404:\n",
      "a: 1.9884, b: -0.9939, loss: 0.0000,  grad a: -0.0015, grad b: 0.0008\n",
      "Itération 405:\n",
      "a: 1.9886, b: -0.9940, loss: 0.0000,  grad a: -0.0014, grad b: 0.0008\n",
      "Itération 406:\n",
      "a: 1.9887, b: -0.9940, loss: 0.0000,  grad a: -0.0014, grad b: 0.0008\n",
      "Itération 407:\n",
      "a: 1.9888, b: -0.9941, loss: 0.0000,  grad a: -0.0014, grad b: 0.0007\n",
      "Itération 408:\n",
      "a: 1.9890, b: -0.9942, loss: 0.0000,  grad a: -0.0014, grad b: 0.0007\n",
      "Itération 409:\n",
      "a: 1.9891, b: -0.9943, loss: 0.0000,  grad a: -0.0014, grad b: 0.0007\n",
      "Itération 410:\n",
      "a: 1.9893, b: -0.9943, loss: 0.0000,  grad a: -0.0014, grad b: 0.0007\n",
      "Itération 411:\n",
      "a: 1.9894, b: -0.9944, loss: 0.0000,  grad a: -0.0013, grad b: 0.0007\n",
      "Itération 412:\n",
      "a: 1.9895, b: -0.9945, loss: 0.0000,  grad a: -0.0013, grad b: 0.0007\n",
      "Itération 413:\n",
      "a: 1.9897, b: -0.9945, loss: 0.0000,  grad a: -0.0013, grad b: 0.0007\n",
      "Itération 414:\n",
      "a: 1.9898, b: -0.9946, loss: 0.0000,  grad a: -0.0013, grad b: 0.0007\n",
      "Itération 415:\n",
      "a: 1.9899, b: -0.9947, loss: 0.0000,  grad a: -0.0013, grad b: 0.0007\n",
      "Itération 416:\n",
      "a: 1.9900, b: -0.9947, loss: 0.0000,  grad a: -0.0013, grad b: 0.0007\n",
      "Itération 417:\n",
      "a: 1.9902, b: -0.9948, loss: 0.0000,  grad a: -0.0012, grad b: 0.0007\n",
      "Itération 418:\n",
      "a: 1.9903, b: -0.9949, loss: 0.0000,  grad a: -0.0012, grad b: 0.0006\n",
      "Itération 419:\n",
      "a: 1.9904, b: -0.9949, loss: 0.0000,  grad a: -0.0012, grad b: 0.0006\n",
      "Itération 420:\n",
      "a: 1.9905, b: -0.9950, loss: 0.0000,  grad a: -0.0012, grad b: 0.0006\n",
      "Itération 421:\n",
      "a: 1.9907, b: -0.9951, loss: 0.0000,  grad a: -0.0012, grad b: 0.0006\n",
      "Itération 422:\n",
      "a: 1.9908, b: -0.9951, loss: 0.0000,  grad a: -0.0012, grad b: 0.0006\n",
      "Itération 423:\n",
      "a: 1.9909, b: -0.9952, loss: 0.0000,  grad a: -0.0012, grad b: 0.0006\n",
      "Itération 424:\n",
      "a: 1.9910, b: -0.9953, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 425:\n",
      "a: 1.9911, b: -0.9953, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 426:\n",
      "a: 1.9912, b: -0.9954, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 427:\n",
      "a: 1.9913, b: -0.9954, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 428:\n",
      "a: 1.9915, b: -0.9955, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 429:\n",
      "a: 1.9916, b: -0.9955, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 430:\n",
      "a: 1.9917, b: -0.9956, loss: 0.0000,  grad a: -0.0011, grad b: 0.0006\n",
      "Itération 431:\n",
      "a: 1.9918, b: -0.9957, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 432:\n",
      "a: 1.9919, b: -0.9957, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 433:\n",
      "a: 1.9920, b: -0.9958, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 434:\n",
      "a: 1.9921, b: -0.9958, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 435:\n",
      "a: 1.9922, b: -0.9959, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 436:\n",
      "a: 1.9923, b: -0.9959, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 437:\n",
      "a: 1.9924, b: -0.9960, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 438:\n",
      "a: 1.9925, b: -0.9960, loss: 0.0000,  grad a: -0.0010, grad b: 0.0005\n",
      "Itération 439:\n",
      "a: 1.9926, b: -0.9961, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 440:\n",
      "a: 1.9927, b: -0.9961, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 441:\n",
      "a: 1.9928, b: -0.9962, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 442:\n",
      "a: 1.9928, b: -0.9962, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 443:\n",
      "a: 1.9929, b: -0.9963, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 444:\n",
      "a: 1.9930, b: -0.9963, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 445:\n",
      "a: 1.9931, b: -0.9964, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 446:\n",
      "a: 1.9932, b: -0.9964, loss: 0.0000,  grad a: -0.0009, grad b: 0.0005\n",
      "Itération 447:\n",
      "a: 1.9933, b: -0.9965, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 448:\n",
      "a: 1.9934, b: -0.9965, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 449:\n",
      "a: 1.9935, b: -0.9965, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 450:\n",
      "a: 1.9935, b: -0.9966, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 451:\n",
      "a: 1.9936, b: -0.9966, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 452:\n",
      "a: 1.9937, b: -0.9967, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 453:\n",
      "a: 1.9938, b: -0.9967, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 454:\n",
      "a: 1.9939, b: -0.9968, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 455:\n",
      "a: 1.9939, b: -0.9968, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 456:\n",
      "a: 1.9940, b: -0.9968, loss: 0.0000,  grad a: -0.0008, grad b: 0.0004\n",
      "Itération 457:\n",
      "a: 1.9941, b: -0.9969, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 458:\n",
      "a: 1.9942, b: -0.9969, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 459:\n",
      "a: 1.9942, b: -0.9970, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 460:\n",
      "a: 1.9943, b: -0.9970, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 461:\n",
      "a: 1.9944, b: -0.9970, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 462:\n",
      "a: 1.9945, b: -0.9971, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 463:\n",
      "a: 1.9945, b: -0.9971, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 464:\n",
      "a: 1.9946, b: -0.9971, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 465:\n",
      "a: 1.9947, b: -0.9972, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 466:\n",
      "a: 1.9947, b: -0.9972, loss: 0.0000,  grad a: -0.0007, grad b: 0.0004\n",
      "Itération 467:\n",
      "a: 1.9948, b: -0.9973, loss: 0.0000,  grad a: -0.0007, grad b: 0.0003\n",
      "Itération 468:\n",
      "a: 1.9949, b: -0.9973, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 469:\n",
      "a: 1.9949, b: -0.9973, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 470:\n",
      "a: 1.9950, b: -0.9974, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 471:\n",
      "a: 1.9951, b: -0.9974, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 472:\n",
      "a: 1.9951, b: -0.9974, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 473:\n",
      "a: 1.9952, b: -0.9975, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 474:\n",
      "a: 1.9952, b: -0.9975, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 475:\n",
      "a: 1.9953, b: -0.9975, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 476:\n",
      "a: 1.9954, b: -0.9976, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 477:\n",
      "a: 1.9954, b: -0.9976, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 478:\n",
      "a: 1.9955, b: -0.9976, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 479:\n",
      "a: 1.9955, b: -0.9976, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 480:\n",
      "a: 1.9956, b: -0.9977, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 481:\n",
      "a: 1.9956, b: -0.9977, loss: 0.0000,  grad a: -0.0006, grad b: 0.0003\n",
      "Itération 482:\n",
      "a: 1.9957, b: -0.9977, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 483:\n",
      "a: 1.9958, b: -0.9978, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 484:\n",
      "a: 1.9958, b: -0.9978, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 485:\n",
      "a: 1.9959, b: -0.9978, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 486:\n",
      "a: 1.9959, b: -0.9978, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 487:\n",
      "a: 1.9960, b: -0.9979, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 488:\n",
      "a: 1.9960, b: -0.9979, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 489:\n",
      "a: 1.9961, b: -0.9979, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 490:\n",
      "a: 1.9961, b: -0.9980, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 491:\n",
      "a: 1.9962, b: -0.9980, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 492:\n",
      "a: 1.9962, b: -0.9980, loss: 0.0000,  grad a: -0.0005, grad b: 0.0003\n",
      "Itération 493:\n",
      "a: 1.9963, b: -0.9980, loss: 0.0000,  grad a: -0.0005, grad b: 0.0002\n",
      "Itération 494:\n",
      "a: 1.9963, b: -0.9981, loss: 0.0000,  grad a: -0.0005, grad b: 0.0002\n",
      "Itération 495:\n",
      "a: 1.9964, b: -0.9981, loss: 0.0000,  grad a: -0.0005, grad b: 0.0002\n",
      "Itération 496:\n",
      "a: 1.9964, b: -0.9981, loss: 0.0000,  grad a: -0.0005, grad b: 0.0002\n",
      "Itération 497:\n",
      "a: 1.9964, b: -0.9981, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 498:\n",
      "a: 1.9965, b: -0.9981, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 499:\n",
      "a: 1.9965, b: -0.9982, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 500:\n",
      "a: 1.9966, b: -0.9982, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 501:\n",
      "a: 1.9966, b: -0.9982, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 502:\n",
      "a: 1.9967, b: -0.9982, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 503:\n",
      "a: 1.9967, b: -0.9983, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 504:\n",
      "a: 1.9967, b: -0.9983, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 505:\n",
      "a: 1.9968, b: -0.9983, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 506:\n",
      "a: 1.9968, b: -0.9983, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 507:\n",
      "a: 1.9969, b: -0.9983, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 508:\n",
      "a: 1.9969, b: -0.9984, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 509:\n",
      "a: 1.9970, b: -0.9984, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 510:\n",
      "a: 1.9970, b: -0.9984, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 511:\n",
      "a: 1.9970, b: -0.9984, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 512:\n",
      "a: 1.9971, b: -0.9985, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 513:\n",
      "a: 1.9971, b: -0.9985, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 514:\n",
      "a: 1.9971, b: -0.9985, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 515:\n",
      "a: 1.9972, b: -0.9985, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 516:\n",
      "a: 1.9972, b: -0.9985, loss: 0.0000,  grad a: -0.0004, grad b: 0.0002\n",
      "Itération 517:\n",
      "a: 1.9972, b: -0.9985, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 518:\n",
      "a: 1.9973, b: -0.9986, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 519:\n",
      "a: 1.9973, b: -0.9986, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 520:\n",
      "a: 1.9973, b: -0.9986, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 521:\n",
      "a: 1.9974, b: -0.9986, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 522:\n",
      "a: 1.9974, b: -0.9986, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 523:\n",
      "a: 1.9974, b: -0.9987, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 524:\n",
      "a: 1.9975, b: -0.9987, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 525:\n",
      "a: 1.9975, b: -0.9987, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 526:\n",
      "a: 1.9975, b: -0.9987, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 527:\n",
      "a: 1.9976, b: -0.9987, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 528:\n",
      "a: 1.9976, b: -0.9987, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 529:\n",
      "a: 1.9976, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 530:\n",
      "a: 1.9977, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 531:\n",
      "a: 1.9977, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 532:\n",
      "a: 1.9977, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0002\n",
      "Itération 533:\n",
      "a: 1.9978, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 534:\n",
      "a: 1.9978, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 535:\n",
      "a: 1.9978, b: -0.9988, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 536:\n",
      "a: 1.9978, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 537:\n",
      "a: 1.9979, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 538:\n",
      "a: 1.9979, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 539:\n",
      "a: 1.9979, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 540:\n",
      "a: 1.9979, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 541:\n",
      "a: 1.9980, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 542:\n",
      "a: 1.9980, b: -0.9989, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 543:\n",
      "a: 1.9980, b: -0.9990, loss: 0.0000,  grad a: -0.0003, grad b: 0.0001\n",
      "Itération 544:\n",
      "a: 1.9980, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 545:\n",
      "a: 1.9981, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 546:\n",
      "a: 1.9981, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 547:\n",
      "a: 1.9981, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 548:\n",
      "a: 1.9981, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 549:\n",
      "a: 1.9982, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 550:\n",
      "a: 1.9982, b: -0.9990, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 551:\n",
      "a: 1.9982, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 552:\n",
      "a: 1.9982, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 553:\n",
      "a: 1.9983, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 554:\n",
      "a: 1.9983, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 555:\n",
      "a: 1.9983, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 556:\n",
      "a: 1.9983, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 557:\n",
      "a: 1.9983, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 558:\n",
      "a: 1.9984, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 559:\n",
      "a: 1.9984, b: -0.9991, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 560:\n",
      "a: 1.9984, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 561:\n",
      "a: 1.9984, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 562:\n",
      "a: 1.9984, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 563:\n",
      "a: 1.9985, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 564:\n",
      "a: 1.9985, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 565:\n",
      "a: 1.9985, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 566:\n",
      "a: 1.9985, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 567:\n",
      "a: 1.9985, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 568:\n",
      "a: 1.9986, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 569:\n",
      "a: 1.9986, b: -0.9992, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 570:\n",
      "a: 1.9986, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 571:\n",
      "a: 1.9986, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 572:\n",
      "a: 1.9986, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 573:\n",
      "a: 1.9986, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 574:\n",
      "a: 1.9987, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 575:\n",
      "a: 1.9987, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 576:\n",
      "a: 1.9987, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 577:\n",
      "a: 1.9987, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 578:\n",
      "a: 1.9987, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 579:\n",
      "a: 1.9987, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 580:\n",
      "a: 1.9988, b: -0.9993, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 581:\n",
      "a: 1.9988, b: -0.9994, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 582:\n",
      "a: 1.9988, b: -0.9994, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 583:\n",
      "a: 1.9988, b: -0.9994, loss: 0.0000,  grad a: -0.0002, grad b: 0.0001\n",
      "Itération 584:\n",
      "a: 1.9988, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 585:\n",
      "a: 1.9988, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 586:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 587:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 588:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 589:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 590:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 591:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 592:\n",
      "a: 1.9989, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 593:\n",
      "a: 1.9990, b: -0.9994, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 594:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 595:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 596:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 597:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 598:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 599:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 600:\n",
      "a: 1.9990, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 601:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 602:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 603:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 604:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 605:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 606:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 607:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 608:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 609:\n",
      "a: 1.9991, b: -0.9995, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 610:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 611:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 612:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 613:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 614:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 615:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 616:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 617:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 618:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 619:\n",
      "a: 1.9992, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0001\n",
      "Itération 620:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 621:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 622:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 623:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 624:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 625:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 626:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 627:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 628:\n",
      "a: 1.9993, b: -0.9996, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 629:\n",
      "a: 1.9993, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 630:\n",
      "a: 1.9993, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 631:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 632:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 633:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 634:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 635:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 636:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 637:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 638:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 639:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 640:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 641:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 642:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 643:\n",
      "a: 1.9994, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 644:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 645:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 646:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 647:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 648:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 649:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 650:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 651:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 652:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 653:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 654:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 655:\n",
      "a: 1.9995, b: -0.9997, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 656:\n",
      "a: 1.9995, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 657:\n",
      "a: 1.9995, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 658:\n",
      "a: 1.9995, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 659:\n",
      "a: 1.9995, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 660:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 661:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 662:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 663:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 664:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 665:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 666:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 667:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 668:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 669:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0001, grad b: 0.0000\n",
      "Itération 670:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 671:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 672:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 673:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 674:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 675:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 676:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 677:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 678:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 679:\n",
      "a: 1.9996, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 680:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 681:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 682:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 683:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 684:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 685:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 686:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 687:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 688:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 689:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 690:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 691:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 692:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 693:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 694:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 695:\n",
      "a: 1.9997, b: -0.9998, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 696:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 697:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 698:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 699:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 700:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 701:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 702:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 703:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 704:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 705:\n",
      "a: 1.9997, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 706:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 707:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 708:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 709:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 710:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 711:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 712:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 713:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 714:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 715:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 716:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 717:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 718:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 719:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 720:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 721:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 722:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 723:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 724:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 725:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 726:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 727:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 728:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 729:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 730:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 731:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 732:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 733:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 734:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 735:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 736:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 737:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 738:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 739:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 740:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 741:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 742:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 743:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 744:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 745:\n",
      "a: 1.9998, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 746:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 747:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 748:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 749:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 750:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 751:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 752:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 753:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 754:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 755:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 756:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 757:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 758:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 759:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 760:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 761:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 762:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 763:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 764:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 765:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 766:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 767:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 768:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 769:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 770:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 771:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 772:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 773:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 774:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 775:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 776:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 777:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 778:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 779:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 780:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 781:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 782:\n",
      "a: 1.9999, b: -0.9999, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 783:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 784:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 785:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 786:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 787:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 788:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 789:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 790:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 791:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 792:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 793:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 794:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 795:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 796:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 797:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 798:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 799:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 800:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 801:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 802:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 803:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 804:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 805:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 806:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 807:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 808:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 809:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 810:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 811:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 812:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 813:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 814:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 815:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 816:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 817:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 818:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 819:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 820:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 821:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 822:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 823:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 824:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 825:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 826:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 827:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 828:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 829:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 830:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 831:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 832:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 833:\n",
      "a: 1.9999, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 834:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 835:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 836:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 837:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 838:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 839:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 840:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 841:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 842:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 843:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 844:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 845:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 846:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 847:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 848:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 849:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 850:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 851:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 852:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 853:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 854:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 855:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 856:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 857:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 858:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 859:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 860:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 861:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 862:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 863:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 864:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 865:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 866:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 867:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 868:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 869:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 870:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 871:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 872:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 873:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 874:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 875:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 876:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 877:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 878:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 879:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 880:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 881:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 882:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 883:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 884:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 885:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 886:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 887:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 888:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 889:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 890:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 891:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 892:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 893:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 894:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 895:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 896:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 897:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 898:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 899:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 900:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 901:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 902:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 903:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 904:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 905:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 906:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 907:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 908:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 909:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 910:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 911:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 912:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 913:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 914:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 915:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 916:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 917:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 918:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 919:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 920:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 921:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 922:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 923:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 924:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 925:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 926:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 927:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 928:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 929:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 930:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 931:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 932:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 933:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 934:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 935:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 936:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 937:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 938:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 939:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 940:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 941:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 942:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 943:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 944:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 945:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 946:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 947:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 948:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 949:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 950:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 951:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 952:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 953:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 954:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 955:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 956:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 957:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 958:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 959:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 960:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 961:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 962:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 963:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 964:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 965:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 966:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 967:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 968:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 969:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 970:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 971:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 972:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 973:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 974:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 975:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 976:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 977:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 978:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 979:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 980:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 981:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 982:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 983:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 984:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 985:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 986:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 987:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 988:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 989:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 990:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 991:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 992:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 993:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 994:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 995:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 996:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 997:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 998:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n",
      "Itération 999:\n",
      "a: 2.0000, b: -1.0000, loss: 0.0000,  grad a: -0.0000, grad b: 0.0000\n"
     ]
    }
   ],
   "source": [
    "a_true = 2.0\n",
    "b_true = -1.0\n",
    "\n",
    "x = torch.rand((10,)) # simulation des données\n",
    "y = a_true * x + b_true\n",
    "\n",
    "a = torch.zeros(1,requires_grad=True) # les objets que l'on va faire converger vers les valeurs recherchées\n",
    "b = torch.zeros(1,requires_grad=True) # requires_grad => un champs gradient est attaché à l'objet crée\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    y_est = a*x + b # je connais x, j'estime y / paramètres estimés courants \n",
    "                    # cette ligne crée un graphe de calcul entre x et y_est mettant en jeu a et b \n",
    "\n",
    "    loss = torch.mean((y_est-y)**2) # je calcule l'erreur entre l'estimation et les valeurs observées\n",
    "    \n",
    "    loss.backward() # je différencie la fonction de perte\n",
    "                    # cela entraine la différentiation automatique de tout le graphe de calcul\n",
    "                    # le gradient est mis à jour dans toutes les variables du graphe / requires_grad = True\n",
    "    \n",
    "    print(f'Itération {i}:')\n",
    "    print(f\"a: {a.item():.4f}, b: {b.item():.4F}, loss: {loss.item():.4f},  grad a: {a.grad.item():.4f}, grad b: {b.grad.item():.4f}\")\n",
    "   \n",
    "    with torch.no_grad(): #je vais effectuer des opérations sur des objets attachés au graphe de calcul \n",
    "                          # mais je ne veux pas que ces opérations entrent dans l'optimisation des paramètres\n",
    "        \n",
    "        a -=  0.1*a.grad # descente de gradient de pas 0.1\n",
    "        b -=  0.1*b.grad\n",
    "        \n",
    "        a.grad.zero_() # je remets à 0 tous les champs gradient des objets\n",
    "        b.grad.zero_()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ee4c4",
   "metadata": {},
   "source": [
    "# Définition d'un réseau de neurones \n",
    "\n",
    "## Complètement à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e529bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonModeleQuiTorche(torch.nn.Module):\n",
    "    def __init__(self,delta_chan=4,verbose=False):\n",
    "        if verbose:\n",
    "            self.print = print\n",
    "        else:\n",
    "            self.print = lambda x:None\n",
    "        self.print('Initialisation classe mère \\n')\n",
    "        torch.nn.Module.__init__(self) \n",
    "        \n",
    "        self.print('\\n Initialisation classe courante \\n')\n",
    "        self.delta_chan=delta_chan\n",
    "        self.learnable_param = torch.nn.Parameter(torch.rand([1,delta_chan,1]))\n",
    "        self.not_learnable_param = torch.rand((1,delta_chan,1))\n",
    "\n",
    "    def __setattr__(self,name,value):\n",
    "        super().__setattr__(name,value)\n",
    "        self.print(f'Enregistrement de: {name} à la valeur {value}')\n",
    "        \n",
    "    def forward(self,x): \n",
    "        #x is [B,input_chan,T]\n",
    "        # output is [B,self.output_chan,T]\n",
    "        x_reduced = torch.mean(x , axis = 1 , keepdim=True)\n",
    "        x_duplicated = torch.tile(x_reduced , dims = (1, self.delta_chan, 1))\n",
    "\n",
    "        y0 = self.learnable_param *x_duplicated \n",
    "        y1 = y0+ self.not_learnable_param\n",
    "\n",
    "        y2 = torch.abs(y1)\n",
    "        \n",
    "        y3 = torch.concat([x, y2], axis=1)\n",
    "        return y3\n",
    "    \n",
    "    def __call__(self,x):\n",
    "    # Défini dans la classe mère\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28e0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enregistrement de: print à la valeur <built-in function print>\n",
      "Initialisation classe mère \n",
      "\n",
      "\n",
      " Initialisation classe courante \n",
      "\n",
      "Enregistrement de: delta_chan à la valeur 4\n",
      "Enregistrement de: learnable_param à la valeur Parameter containing:\n",
      "tensor([[[0.3878],\n",
      "         [0.5882],\n",
      "         [0.5428],\n",
      "         [0.1798]]], requires_grad=True)\n",
      "Enregistrement de: not_learnable_param à la valeur tensor([[[0.5433],\n",
      "         [0.7266],\n",
      "         [0.5915],\n",
      "         [0.1114]]])\n"
     ]
    }
   ],
   "source": [
    "mon_modele=MonModeleQuiTorche(delta_chan=4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3040ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape torch.Size([5, 1, 100]), output shape torch.Size([5, 5, 100]), second output shape torch.Size([5, 9, 100])\n"
     ]
    }
   ],
   "source": [
    "x= torch.rand(5,1,100)\n",
    "y = mon_modele(x)\n",
    "z = mon_modele(y)\n",
    "print(f'Input shape {x.shape}, output shape {y.shape}, second output shape {z.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f219586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramètres du modèle : \n",
      " {'learnable_param': Parameter containing:\n",
      "tensor([[[0.3878],\n",
      "         [0.5882],\n",
      "         [0.5428],\n",
      "         [0.1798]]], requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "print(f'Paramètres du modèle : \\n {mon_modele._parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd46f0b",
   "metadata": {},
   "source": [
    "## En enchaînant des couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55293b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_modele_sequentiel = torch.nn.Sequential( \n",
    "  MonModeleQuiTorche(4),\n",
    "  MonModeleQuiTorche(5),\n",
    "  MonModeleQuiTorche(6)  )\n",
    "\n",
    "mon_modele_sequentiel(torch.rand(5,1,100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b573aa",
   "metadata": {},
   "source": [
    "### Quand il y en a beaucoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44022367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 86, 100])\n"
     ]
    }
   ],
   "source": [
    "mlp = torch.nn.Sequential(*[MonModeleQuiTorche(4+i) for i in range(10)])\n",
    "print(mlp((torch.rand(5,1,100))).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d8d93",
   "metadata": {},
   "source": [
    "## Couches linéaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4abbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(in_features=40,\n",
    "                         out_features=100\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad515649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is torch.Size([10, 40])\n",
      "Output shape is torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([10, 40])\n",
    "print(f'Input shape is {x.shape}')\n",
    "y = linear(x)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71bc605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres à apprendre :  4100\n"
     ]
    }
   ],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in linear.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259a108",
   "metadata": {},
   "source": [
    "### Couches linéaires sur un signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57dbfef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is torch.Size([10, 4, 10])\n",
      "x flatten shape is : torch.Size([10, 40])\n",
      "Output shape is torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([10, 4, 10]) #[B, C, T]\n",
    "print(f'Input shape is {x.shape}')\n",
    "x_flat = torch.flatten(x, start_dim = 1 , end_dim=2)\n",
    "print(f'x flatten shape is : {x_flat.shape}')\n",
    "y = linear(x_flat)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f9c92",
   "metadata": {},
   "source": [
    "## Couches de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee4507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv1d(in_channels=4,  # entrée [B,1,T]\n",
    "                       out_channels=10, # sortie [B,10,T']\n",
    "                       kernel_size=5, \n",
    "                       stride=1,       # T' = T//2\n",
    "                       padding='same', # idem (kernel_size-1)//2 \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b5d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is torch.Size([10, 4, 100])\n",
      "Output shape is torch.Size([10, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([10, 4, 100]) #[B, C, T]\n",
    "print(f'Input shape is {x.shape}')\n",
    "\n",
    "y = conv(x)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce9c6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres à apprendre :  210\n"
     ]
    }
   ],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in conv.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b270a",
   "metadata": {},
   "source": [
    "### Convolutions séparables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0841790",
   "metadata": {},
   "outputs": [],
   "source": [
    "depthwise = torch.nn.Conv1d(in_channels=4,  # entrée [B,1,T]\n",
    "                           out_channels=4,  # sortie [B,4,T]\n",
    "                           groups= 4,       # correspond à in_channels\n",
    "                           kernel_size=5,  \n",
    "                           stride=1,  \n",
    "                           padding='same'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fcb91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise = torch.nn.Conv1d(in_channels=4,  # entrée [B,1,T]\n",
    "                           out_channels=10,  # sortie [B,4,T]\n",
    "                           kernel_size=1,   \n",
    "                           stride=1, \n",
    "                           padding='same'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35336750",
   "metadata": {},
   "outputs": [],
   "source": [
    "separable_convolution = torch.nn.Sequential(depthwise,\n",
    "                                            pointwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e2c53a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is torch.Size([10, 4, 100])\n",
      "Output shape is torch.Size([10, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([10, 4, 100]) #[B, C, T]\n",
    "print(f'Input shape is {x.shape}')\n",
    "\n",
    "y = separable_convolution(x)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e050754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres à apprendre :  74\n"
     ]
    }
   ],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in separable_convolution.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e59b2",
   "metadata": {},
   "source": [
    "## Couches récurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7da5409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent = torch.nn.RNN(input_size=6, # x is [B, T , input_size]\n",
    "                        hidden_size =15, # h is [B,T, hidden_size]\n",
    "                        num_layers =1, # par défaut\n",
    "                        batch_first=True,  # pour que la première dimension soit bien le batch\n",
    "                        bidirectional= False, # par défaut\n",
    "                         bias= False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49291ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand([20, 100,6])\n",
    "h , h_layers_end = recurrent(x)\n",
    "print(h.shape)\n",
    "print(h_layers_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in recurrent.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47920dd1",
   "metadata": {},
   "source": [
    "###  Réseau récurrent avec deux couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent = torch.nn.RNN(input_size=6, # x is [B, T , input_size]\n",
    "                        hidden_size =15, # h is [B,T, hidden_size]\n",
    "                        num_layers =4, # h de la couche 0 devient le x de la couche 1 etc.\n",
    "                        batch_first=True,  # pour que la première dimension soit bien le batch\n",
    "                        bidirectional= False, # valeur par défaut\n",
    "                        bias= False\n",
    "\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3981ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand([20, 100,6])\n",
    "h , h_layers_end = recurrent(x)\n",
    "print(h.shape)\n",
    "print(h_layers_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a09488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in recurrent.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee0b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a122d0",
   "metadata": {},
   "source": [
    "###  Réseau récurrent bidirectionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db232a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent = torch.nn.RNN(input_size=6, # x is [B, T , input_size]\n",
    "                        hidden_size =15, # h is [B,T, hidden_size]\n",
    "                        num_layers =7, # h de la couche 0 devient le x de la couche 1 etc.\n",
    "                        batch_first=True,  # pour que la première dimension soit bien le batch\n",
    "                        bidirectional= True, # cf Algorithme forward backward\n",
    "                         bias = False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand([20, 100,6])\n",
    "h , h_layers_end = recurrent(x)\n",
    "print(h.shape)\n",
    "print(h_layers_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in recurrent.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6834d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(6+15)*15*2 + 6*(15*2 + 15 )*(15)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0faad",
   "metadata": {},
   "source": [
    "# Pipeline d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_data):\n",
    "        ...\n",
    "    def __len__(self): #returns int\n",
    "        ...\n",
    "    def __getitem__(self,i): #returns (data_i,label_i)\n",
    "        ...\n",
    "\n",
    "dataset = MyDataset(...)\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86990f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class metric_logger:\n",
    "    \n",
    "    def __init__(self,...):\n",
    "        ...\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        ... \n",
    "        \n",
    "    def update_metrics(self, batch_x,batch_y_true,batch_y_pred):\n",
    "        ...\n",
    "    \n",
    "        return {'metric0':...,\n",
    "               'metric1':...\n",
    "               }\n",
    "    \n",
    "    def log(self):\n",
    "        ...\n",
    "\n",
    "device = 'cpu' # set so 'cuda:xx' if you have a GPU, xx is GPU index\n",
    "model = ... \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "metric_logger_train = metric_logger(...)\n",
    "metric_logger_valid = metric_logger(...)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    metric_logger_train.reset()\n",
    "    \n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_y_predicted = model(batch_x)\n",
    "        \n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        \n",
    "        metric_logger_train.log(batch_x,batch_y,batch_y_predicted)\n",
    "        \n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x)  \n",
    "            \n",
    "        metric_logger_valid.log(batch_x,batch_y,batch_y_predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
